# -*- coding=utf8 -*-
# Aug 13, 2022
# For testing codes
# Created by Wei Yin

import torchtext.datasets as datasets
import spacy
import io
import os
from real_example import load_tokenizers, tokenize, yield_tokens, getData, build_vocabulary, load_vocab
from torchtext.vocab import build_vocab_from_iterator
import torch
from torch.nn.functional import log_softmax, pad

train, val, test =getData()
def tokenize_de(text):
    return tokenize(text, spacy_de)
def tokenize_en(text):
    return tokenize(text, spacy_en)

spacy_de, spacy_en=load_tokenizers()
vocab_src, vocab_tgt=torch.load("vocab.pt")
bs_id = torch.tensor([0])  # <s> token id
eos_id = torch.tensor([1])  # </s> token id
tgt=torch.cat(
    [
        bs_id,
        torch.tensor(
            vocab_tgt(tokenize_en(test[0][1])),
            dtype=torch.int64,
        ),
        eos_id,
    ],
    0,
)
tgt_pad=pad(
    tgt,
    (
        1,
        3,
    ),
    value=2,
)
for i in range(len(tgt_pad)):
    print(vocab_tgt.lookup_token(tgt_pad[i].data))





